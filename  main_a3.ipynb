{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, COMP 472\n",
    "## Due October 19th, 2020\n",
    "## Use of Python 3.8\n",
    "## Marc Vicuna, 40079109, El Hassan Ait Ouaziz, 26791573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'data/covid_training.tsv'\n",
    "data = pd.read_csv(training_file, sep='\\t', usecols=[1,2])\n",
    "documents = data['text'].values\n",
    "labels = data['q1_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document(doc, lab, ov):\n",
    "    doc = doc.casefold()\n",
    "    for word in doc.split(' '):\n",
    "        if word in ov:\n",
    "            ov[word] += lab\n",
    "        else:\n",
    "            ov[word] = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov = {}\n",
    "priors = np.zeros(2)\n",
    "for doc, lab in zip(documents, labels):\n",
    "    if lab == 'yes':\n",
    "        priors[0] += 1\n",
    "        add_document(doc, np.array([1,0], dtype = 'i4'), ov)\n",
    "    else:\n",
    "        priors[1] += 1\n",
    "        add_document(doc, np.array([0,1], dtype = 'i4'), ov)\n",
    "priors = priors/np.sum(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv = {}\n",
    "ov_sum = np.zeros(2, dtype = 'i4')\n",
    "fv_sum = np.zeros(2, dtype = 'i4')\n",
    "for word, freq in ov.items():\n",
    "    ov_sum += freq\n",
    "    if np.sum(freq) > 1:\n",
    "        fv[word] = freq\n",
    "        fv_sum += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing\n",
    "d = 0.01\n",
    "# vocabulary size\n",
    "V = len(ov)\n",
    "# conditional probability on ov\n",
    "for word, freq in ov.items():\n",
    "    ov[word] = np.log10(np.array([(freq[0]+d)/(ov_sum[0]+d*V),(freq[1]+d)/(ov_sum[1]+d*V)]))\n",
    "# conditional probability on fv\n",
    "for word, freq in fv.items():\n",
    "    fv[word] = np.log10(np.array([(freq[0]+d)/(fv_sum[0]+d*V),(freq[1]+d)/(fv_sum[1]+d*V)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test data\n",
    "test_file = 'data/covid_test_public.tsv'\n",
    "data = pd.read_csv(test_file, sep='\\t',header = None, usecols=[0,1,2])\n",
    "test_IDs = np.array(data[0].values, dtype = 'i4')\n",
    "test_documents = data[1].values\n",
    "test_labels = data[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_document(doc, v, priors):\n",
    "    prob = np.log10(priors.copy())\n",
    "    doc = doc.casefold()\n",
    "    for word in doc.split(' '):\n",
    "        if word in v:\n",
    "            prob += v[word]\n",
    "    if np.argmax(prob) == 0:\n",
    "        return 'yes', prob[0]\n",
    "    else:\n",
    "        return 'no', prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yes', -101.6057968039355)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_document(test_documents[0], ov, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Trace files\n",
    "def trace(filename, IDs, documents, labels, v, priors):\n",
    "    f = open(filename, 'w')\n",
    "    for ID, doc, true_lab in zip(IDs, documents, labels):\n",
    "        estim_lab, prob = evaluate_document(doc, v, priors)\n",
    "        if estim_lab == true_lab:\n",
    "            match = 'correct'\n",
    "        else:\n",
    "            match = 'wrong'\n",
    "        f.write('{}  {}  {:.2E}  {}  {}\\n'.format(ID, estim_lab, 10**prob, true_lab, match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace('trace/trace_NB-BOW-OV.txt', test_IDs, test_documents, test_labels, ov, priors)\n",
    "trace('trace/trace_NB-BOW-FV.txt', test_IDs, test_documents, test_labels, fv, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Evaluation files\n",
    "def evalation(input_file, output_file):\n",
    "    data = pd.read_csv(input_file, sep='  ', header = None, usecols=[1,3,4], engine='python')\n",
    "    estim_l = data[1].values\n",
    "    true_l = data[3].values\n",
    "    match = data[4].values\n",
    "    accuracy, precision, recall = 0.0, np.array([0,0], dtype = 'f4'), np.array([0,0], dtype = 'f4')\n",
    "    for est, tru, mat in zip(estim_l, true_l, match):\n",
    "        if mat == 'correct':\n",
    "            accuracy += 1\n",
    "            if est == 'yes':\n",
    "                precision[0] += 1\n",
    "            else:\n",
    "                precision[1] += 1\n",
    "            if tru == 'yes':\n",
    "                recall[0] += 1\n",
    "            else:\n",
    "                recall[1] += 1\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    accuracy /= len(match)\n",
    "    \n",
    "    precision /= len(match)\n",
    "    recall /= len(match)\n",
    "    F1 = [2/(precision[0]**-1+recall[0]**-1),2/(precision[1]**-1+recall[1]**-1)]\n",
    "    f = open(output_file, 'w')\n",
    "    f.write('{:.4}\\n'.format(accuracy))\n",
    "    for metric in [precision, recall, F1]:\n",
    "        f.write('{:.4}  {:.4}\\n'.format(metric[0], metric[1]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.  4.]\n",
      "[31.  4.]\n",
      "[32.  3.]\n",
      "[32.  3.]\n"
     ]
    }
   ],
   "source": [
    "evalation('trace/trace_NB-BOW-OV.txt', 'evaluation/eval_NB-BOW-OV.txt')\n",
    "evalation('trace/trace_NB-BOW-FV.txt', 'evaluation/eval_NB-BOW-FV.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
