{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, COMP 472\n",
    "## Due December 13th, 2020\n",
    "## Use of Python 3.8\n",
    "## Marc Vicuna, 40079109, El Hassan Ait Ouaziz, 26791573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility for text processing\n",
    "def read_train_data(train_file):\n",
    "    data = pd.read_csv(training_file, sep='\\t', usecols=[1,2])\n",
    "    return data['text'].values, data['q1_label'].values\n",
    "def read_test_data(test_file):\n",
    "    data = pd.read_csv(test_file, sep='\\t',header = None, usecols=[0,1,2])\n",
    "    return np.array(data[0].values, dtype = 'i4'), data[1].values, data[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the vocubulary through training. Documents and labels of dataset, d is the smoothing\n",
    "def make_vocabulary(documents, labels, d = 0.01):\n",
    "    ov = {}\n",
    "    priors = np.zeros(2)\n",
    "    for doc, lab in zip(documents, labels):\n",
    "        if lab == 'yes':\n",
    "            priors[0] += 1\n",
    "            add_document(doc, np.array([1,0], dtype = 'i4'), ov)\n",
    "        else:\n",
    "            priors[1] += 1\n",
    "            add_document(doc, np.array([0,1], dtype = 'i4'), ov)\n",
    "    priors = priors/np.sum(priors)\n",
    "    #Construct fv vocabulary, find word count for each category, for each vocabulary.\n",
    "    fv = {}\n",
    "    ov_sum = np.zeros(2, dtype = 'i4')\n",
    "    fv_sum = np.zeros(2, dtype = 'i4')\n",
    "    for word, freq in ov.items():\n",
    "        ov_sum += freq\n",
    "        if np.sum(freq) > 1:\n",
    "            fv[word] = freq\n",
    "            fv_sum += freq\n",
    "    # vocabulary size\n",
    "    V_ov = len(ov)\n",
    "    V_fv = len(fv)\n",
    "    # conditional probability on ov\n",
    "    for word, freq in ov.items():\n",
    "        ov[word] = np.log10(np.array([(freq[0]+d)/(ov_sum[0]+d*V_ov),(freq[1]+d)/(ov_sum[1]+d*V_ov)]))\n",
    "    # conditional probability on fv\n",
    "    for word, freq in fv.items():\n",
    "        fv[word] = np.log10(np.array([(freq[0]+d)/(fv_sum[0]+d*V_fv),(freq[1]+d)/(fv_sum[1]+d*V_fv)]))\n",
    "    return ov, fv, priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sub-function\n",
    "# add one document to update vocubulary\n",
    "def add_document(doc, lab, ov):\n",
    "    doc = doc.casefold()\n",
    "    for word in doc.split(' '):\n",
    "        if word in ov:\n",
    "            ov[word] += lab\n",
    "        else:\n",
    "            ov[word] = lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "# Generate Trace files, tests the algorithm and prints results in the file.\n",
    "def trace(filename, IDs, documents, labels, v, priors):\n",
    "    f = open(filename, 'w')\n",
    "    for ID, doc, true_lab in zip(IDs, documents, labels):\n",
    "        estim_lab, prob = evaluate_document(doc, v, priors)\n",
    "        if estim_lab == true_lab:\n",
    "            match = 'correct'\n",
    "        else:\n",
    "            match = 'wrong'\n",
    "        f.write('{}  {}  {:.2E}  {}  {}\\n'.format(ID, estim_lab, 10**prob, true_lab, match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing subfunction, evaluates a single document\n",
    "def evaluate_document(doc, v, priors):\n",
    "    prob = np.log10(priors.copy())\n",
    "    doc = doc.casefold()\n",
    "    for word in doc.split(' '):\n",
    "        if word in v:\n",
    "            prob += v[word]\n",
    "    if np.argmax(prob) == 0:\n",
    "        return 'yes', prob[0]\n",
    "    else:\n",
    "        return 'no', prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Evaluation files\n",
    "def evalation(input_file, output_file):\n",
    "    data = pd.read_csv(input_file, sep='  ', header = None, usecols=[1,3,4], engine='python')\n",
    "    estim_l = data[1].values\n",
    "    true_l = data[3].values\n",
    "    match = data[4].values\n",
    "    accuracy, precision, recall = 0.0, np.array([0,0], dtype = 'f4'), np.array([0,0], dtype = 'f4')\n",
    "    for est, tru, mat in zip(estim_l, true_l, match):\n",
    "        if mat == 'correct':\n",
    "            accuracy += 1\n",
    "            if est == 'yes':\n",
    "                precision[0] += 1\n",
    "            else:\n",
    "                precision[1] += 1\n",
    "            if tru == 'yes':\n",
    "                recall[0] += 1\n",
    "            else:\n",
    "                recall[1] += 1\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    accuracy /= len(match)\n",
    "    \n",
    "    precision /= len(match)\n",
    "    recall /= len(match)\n",
    "    F1 = [2/(precision[0]**-1+recall[0]**-1),2/(precision[1]**-1+recall[1]**-1)]\n",
    "    f = open(output_file, 'w')\n",
    "    f.write('{:.4}\\n'.format(accuracy))\n",
    "    for metric in [precision, recall, F1]:\n",
    "        f.write('{:.4}  {:.4}\\n'.format(metric[0], metric[1]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.  4.]\n",
      "[31.  4.]\n",
      "[32.  3.]\n",
      "[32.  3.]\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "# process data\n",
    "train_file = 'data/covid_training.tsv'\n",
    "train_documents, train_labels = read_train_data(train_file)\n",
    "\n",
    "test_file = 'data/covid_test_public.tsv'\n",
    "test_IDs, test_documents, test_labels = read_test_data(test_file)\n",
    "\n",
    "# make model\n",
    "ov, fv, priors = make_vocabulary(train_documents, train_labels)\n",
    "\n",
    "# test model\n",
    "trace('trace/trace_NB-BOW-OV.txt', test_IDs, test_documents, test_labels, ov, priors)\n",
    "trace('trace/trace_NB-BOW-FV.txt', test_IDs, test_documents, test_labels, fv, priors)\n",
    "\n",
    "# evaluate the model\n",
    "evalation('trace/trace_NB-BOW-OV.txt', 'evaluation/eval_NB-BOW-OV.txt')\n",
    "evalation('trace/trace_NB-BOW-FV.txt', 'evaluation/eval_NB-BOW-FV.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
